\title{A Statistical Analysis of Chicago Traffic}
\author{Tristan Rasmussen}
\date{\today}

\documentclass[12pt]{article}
\usepackage{multicol}
\twocolumn
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{hyperref}
\usepackage{lscape}
\usepackage{wrapfig}
\usepackage[margin=0.5in]{geometry}

\begin{document}
\maketitle
%\begin{multicols}{2}
\section{Introduction}
Everybody hates traffic. Some studies have show that traffic ends up costing Americans over 120 billion dollars a year \cite{costoftraffic}. However, before trying to address the problem of traffic it is imperative that we form accurate statistical models of traffic flow and the variables that affect it. In this paper, we perform both a parametric and nonparametric analysis of traffic in the city of Chicago.
\subsection{Goals}
The ultimate goal of this paper is to build a statistical model for traffic in the city of Chicago. To do this we will explore two components of building a model both parametrically and nonparametrically. First we will explore how we can estimate the probability density of traffic speeds; this is useful for visualizing the data, as well as future statistical calculations. Afterwards we will look into building a regression model for the data, with the goal of accurately predicting the traffic given a set of independent variables.
\section{Description of the Data}
For the purposes of this paper we will be examining Chicago traffic data conjunction with data about sports games in the city. These data sets were obtained separately and then combine together.
\subsection{Chicago Congestion}
The main data set of interest is a record of estimated traffic speeds in 29 regions of Chicago gathered from 03/12/2011 until 03/26/2013 \cite{regiondataset}. For the purposes of this data set the City of Chicago is partitioned into 29 regions, and each data point is the speed for a specific region at a given date and time. Speeds were calculated by sending GPS probes to CTA buses that were operating in the specific region at the given time. Measurements were taken every 10 minutes and a record was kept of the speed, in addition to the number of buses in the region and the number of valid GPS reads that were used when calculating the speed.

In addition to data about regions, the City of Chicago provides more detailed data about 1250 individual segments of road \cite{segmentdataset}. Some preliminary analysis was done of this data set, but it had a much more restricted time frame (2013-1-15 - 2013-2-28) and so analysis was eventually abandoned. However, we will use this data set to try to explain some of the phenomenon observed in the region data.
\subsection{Chicago Sports Games}
The second data set of interest was a collection of all sports games that have happened in Chicago during the period of interest (03/12/2011 - 03/26/2013). We predicted that sports games would have a significant impact on traffic and so would be a useful feature to add to the data set. Unfortunately, we were unable to find a single comprehensive list of all sports games in Chicago and were forced to go to individual sites. We gathered the dates of every Bulls \cite{bullsdata}, Bears \cite{bearsdata}, Whitesox \cite{whitesoxdata}, Cubs \cite{cubsdata} and Blackhawks \cite{blackhawksdata} game in the time period and created a data set where each point was a date and the games that occurred on that date.
\subsection{Data Preprocessing}
Once the above datasets were gathered they were combine into a single data set that contained all of the information we wanted. Each timestamp was broken down into year, month, day, hour, minute, day of week, and a flag indicating if it was a weekend or not. In addition a day label was added that indexed the days sequentially. Finally we added a column that was true if there was any sports game on a day and false otherwise. Once all of this preliminary processing was done, we cleaned up the data set by removing any data points that had invalid speeds (either caused by having 0 bus reads or by having a speed over 40 mph when most roads have a speed limit of 30 mph). 
\subsection{Preliminary Exploration and Visualization}

Having discussed the data that we examined we will now provide several visualizations to give the reader a better sense of the data set. First, Figure \ref{speedvshour13} shows the speed as a function of hour in region 13 (Down town Chicago Loop); the drop in during the day, especially around 5 PM should make intutive sense to anybody who has worked in down town Chicago. We have also constructed a lightewight web application to interact with the results of this analysis \cite{trafficapp}; Figure \ref{appscreenshot} shows a screen shot of this application to illustrate the regions of interest and average traffic patters at 5 PM. Finally, Figure \ref{region11games} illustrates the average speed in Region 11 (containing the United Center) when there was no sports game (red Xs) and when there was a Bulls or Blackhawks game (black Os). Most noticeably there is a significant dip in the later evening that seems to be the result of the game.

\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{../plots/speedvshour13}
\caption{Speed vs Hour in Region 13}
\label{speedvshour13}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{../plots/region11games}
\caption{Speed vs Hour in Region 11 with and without sports games}
\label{region11games}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{../plots/trafficapp}
\caption{Screen Shot of webapp showing regions and predicted traffic}
\label{appscreenshot}
\end{figure}

\section{Parametric Analysis}
In this section we apply parametric methods to the traffic dataset. All of the numeric results from this section and the next are collected in Table \ref{resultstable}.
\subsection{Density Estimation}
To perform a parametric density estimate we necessarily must make some assumptions about the underlying distribution. In our analysis we assumed that the data would be distributed normally. Under this assumption we could simply fit a distribution by taking the sample mean and sample standard deviation. These numbers are included in Table \ref{resultstable} with 95\% confidence intervals for both.

In addition to computing the above distributions, we generated QQ plots to examine how well the data actually conformed to the normality assumptions. Many of these plots exhibited significant non linearity (indicating departure from normality), but we were unable to match the behaviour to any known distribution. As such, we chose to stay with the assumption of normality, even though the fits produced were suboptimal.
\subsection{Regression Analysis}
Having examined the parametric density estimates we can now turn to the second goal for this paper: creating accurate models around for traffic speed. To create a parametric model we tried two different classes of  models: linear models which make the assumption that the response is linear in terms of the input and cubic models which assume the response is cubic. We chose to form linear models as a baseline for our models: if the other models couldn't significantly outperform a linear model then they were clearly not worthwhile. We also formed cubic models because we recognized that the data was clearly nonlinear in most variables; we settled on cubic models after visual inspection indicated that generally the data was not excessively wobbly and in order to reduce the effects of over fitting.

In both the linear and the cubic cases we first partitioned the data into training and test sets. the test set was a 25\% random sample of the whole data and the training set was the remaining segment. To make sure runs were reproducible we added the ability to seed the random number generator to ensure that test/training sets would be consistent across runs if needed. arning: Wrapping figures in LaTex will require a lot of manual adjustment of your document. There are several packages available for the task, but none of them works perfectly. Before you make the choice of including figures with text wrapping in your document, make sure you have considered all the options. For example, you could use a layout with two columns for your documents and have no text-wrapping at all.

After we had partitioned the data we performed model selection using Akaike information criterion (AIC). Due to time and computation constraints we were unable to examine all possible models; instead we hand selected a set of models that we expected to fit the data well. We then trained each of these models on the training set and chose the model with the lowest AIC. Finally we ran the best model against the test set and computed the testing error. Reported above in Table \ref{resultstable} we include the training error, test error, and computed $r^2$ for the best linear and cubic models. 
\section{Nonparametric Analysis}
In this section we relaxed the assumptions made above and performed non parametric analysis on the traffic data set. Once again, the results are all summarized in Table \ref{resultstable}.
\subsection{Density Estimation}
We saw above that a parametric density estimate proved to be a poor tool for estimating the speed data. After this we attempted to perform a nonparametric density estimate using a kernel density estimator (KDE). We trained a KDE on each region and computed 95\% confidence bands.
\subsection{Regression Analysis}
In an attempt to improve on the parametric models we wanted to fit a nonparametric model to the data. To do this we chose to model the data using a higher dimensional local linear regression. This model makes very weak assumptions about the underlying function (depending upon the desired convergence rate either that it is Lipschitz Continuous or that it has two bounded derivatives). Originally we were considering using a Generalized Additive Model (GAM) since local linear regressions begin to degrade in higher dimensions. However, several of the models that we wanted to experiment with involved interaction terms which GAMs explicitly disallow. As such, we decided to stick with a local linear regression, despite the curse of dimensionality.

Our methodology for generating a local linear model was very similar to our methodology in the parametric case: we partitioned the data set, did model selection using AIC, and computed the training/test error. The one addition is that we implemented cross validation for selecting the optimal bandwidth. This cross validation was used in both model selection and final computations. During model selection, we chose the bandwidth that produced the minimum leave one out cross validation score and used the AIC value for this fit (we could have used leave one out cross validation instead of AIC during model selection, but we wanted to keep it consistent with our parametric analysis, and the two are asymptotically equivalent). Table \ref{resultstable} contains the test and training error for the optimal fit for each region.
\section{Discussion}
\subsection{Density Estimation}
If we compare the parametric and nonparametric density estimates it should be clear that the parametric estimate is clearly insufficient for this purpose. Not only are the tails too thin for the normal distribution, it often covers up interesting behaviour such as the flat bump in region 13. 
\subsection{Regression Analysis}
Looking at the results from both parametric and non parametric analysis it seems that there were two broad categories of regions: one group of regions was fit relatively well by the simple models in this paper (generally leading to a $r^2$ between .4 and 0.5 for a non linear model), and the other group on which the models failed miserably. Most of the regions fell into the first group, but regions 2, 16, 17, 22, 25, 27, 28, and 29 all fell in the second group, all of the regions except region 16, in the second group fall on the geographic periphery of the measurements. [ some conclusion here]. 

Region 16 is interesting because it is near the center of Chicago and so one might expect it to have a good fit. However, if we look at the speed vs hour in region 16, we can see that it has almost no hourly variation and that the distribution is significantly shifted to the right. From this we might conclude that speeds in 16 are primarily influenced by buses travelling along Lake Shore Drive, and that the factors that we have examined here are more suited for smaller streets.

It is almost universally true that the cubic models outperformed the linear models. This is to be expected since cubic models are a superset of linear models. The effectiveness of the nonparametric models is more complicated. In regions near the Loop (12, 13, 14, 15) the nonparametric model significantly outperformed the cubic model on both the test and the train data. However, in other regions the non parametric model performed at about the same level, or slightly worse than the cubic models. This could be indicative of the fact that traffic patterns are more complex in metropolitan areas.
\subsection{Future Work}
In this analysis we chose a relatively small subset of factors that we believed would impact traffic speed (time factors and sports games). However, it has become apparent that these factors are not sufficient to fully capture the variability of traffic speeds. The next step would be to expand the number of factors that are included in the analysis; possible factors could include weather (temperature, precipitation, etc.), gas prices, etc. Additionally the data set that we worked with contained only two years of data; a more expansive data set would allow us to better analyze long term trends and factors (population, unemployment, etc.). 

The analyses above was carried out ignoring the timeseries nature of the data. Another possible project would be to apply methods that take into account the fact that there is a time based dependence in the data.
\section{Biblography}
\begin{thebibliography}{9}

\bibitem{costoftraffic}
  \url{http://www.cebr.com/reports/the-future-economic-and-environmental-costs-of-gridlock/}

\bibitem{regiondataset}
  \url{https://data.cityofchicago.org/Transportation/Chicago-Traffic-Tracker-Historical-Congestion-Esti/emtn-qqdi}
  
\bibitem{segmentdataset}
  \url{https://data.cityofchicago.org/Transportation/Chicago-Traffic-Tracker-Historical-Congestion-Esti/77hq-huss}

\bibitem{bullsdata}
  \url{http://www.landofbasketball.com/results/2010_2011_scores_full.htm}

\bibitem{cubsdata}
  \url{http://chicago.cubs.mlb.com/schedule/sortable.jsp?c_id=chc&year=2011}

\bibitem{bearsdata}
  \url{http://www.repole.com/sun4cast/data.html#dataprior}
	
\bibitem{whitesoxdata}
  \url{http://www.baseball-almanac.com/teamstats/schedule.php?y=2013&t=CHA}

\bibitem{blackhawksdata}
  \url{http://www.hockey-reference.com/teams/CHI/2011_games.html}
  
\bibitem{trafficapp}
  \url{chicago-traffic.mooo.com}

\end{thebibliography}
%\end{multicols}
\begin{landscape}
\begin{table}
\small
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline
ID & $\mu$ & $\sigma^2$ & L Train & NL Train & NP Train & L Test & NL Test & NP Test & L $r^2$ & NL $r^2$\\\hline
1&24.70 $\pm$ 2.23E-02& 8.65 $\pm$ 2.24E-03&6.32&4.42&4.86&6.37&4.38&4.92&0.27&0.49\\\hline
2&29.02 $\pm$ 2.01E-02& 7.31 $\pm$ 1.53E-03&6.93&6.67&6.14&7.09&6.87&6.32&0.05&0.08\\\hline
3&24.97 $\pm$ 2.51E-02& 12.04 $\pm$ 3.96E-03&8.76&5.52&5.04&8.78&5.49&4.84&0.27&0.54\\\hline
4&23.06 $\pm$ 2.54E-02& 11.79 $\pm$ 3.97E-03&7.99&4.72&4.5&7.99&4.72&4.51&0.32&0.6\\\hline
5&25.90 $\pm$ 2.31E-02& 9.40 $\pm$ 2.60E-03&7.13&4.74&4.54&7.13&4.77&4.58&0.24&0.49\\\hline
6&24.29 $\pm$ 2.62E-02& 12.73 $\pm$ 4.54E-03&9.31&6.37&5.78&9.65&6.61&5.8&0.26&0.5\\\hline
7&23.83 $\pm$ 2.42E-02& 10.60 $\pm$ 3.23E-03&7.78&5.04&5.02&8.05&5.26&5.04&0.26&0.52\\\hline
8&22.66 $\pm$ 2.74E-02& 14.44 $\pm$ 5.63E-03&9.98&6.56&5.29&9.77&6.35&5.33&0.31&0.55\\\hline
9&25.76 $\pm$ 2.32E-02& 9.21 $\pm$ 2.59E-03&7.46&5.22&5.32&7.56&5.33&5.4&0.19&0.43\\\hline
10&24.52 $\pm$ 2.17E-02& 9.12 $\pm$ 2.23E-03&6.88&5.5&5.6&7.05&5.74&5.47&0.24&0.39\\\hline
11&24.29 $\pm$ 2.80E-02& 15.23 $\pm$ 6.23E-03&11.34&6.8&6&11.27&6.75&6.27&0.26&0.55\\\hline
12&20.78 $\pm$ 3.13E-02& 18.45 $\pm$ 9.43E-03&12.35&7.21&5.22&12.23&7.14&5.28&0.33&0.61\\\hline
13&19.07 $\pm$ 3.19E-02& 19.72 $\pm$ 1.04E-02&14&6.46&3.48&14.19&6.76&3.5&0.29&0.67\\\hline
14&26.58 $\pm$ 2.66E-02& 13.19 $\pm$ 4.88E-03&10.56&6.96&5.16&10.52&6.95&5.29&0.2&0.47\\\hline
15&27.51 $\pm$ 2.69E-02& 13.84 $\pm$ 5.21E-03&10.89&8.26&6.42&10.9&8.28&6.38&0.21&0.4\\\hline
16&29.11 $\pm$ 4.09E-02& 28.97 $\pm$ 2.53E-02&23.62&22.17&23.04&23.59&21.98&22.55&0.18&0.23\\\hline
17&31.36 $\pm$ 4.61E-02& 35.10 $\pm$ 3.89E-02&32.24&24.12&20.98&33.14&24.65&20.96&0.07&0.31\\\hline
18&27.04 $\pm$ 2.36E-02& 10.78 $\pm$ 3.14E-03&8.13&5.39&4.37&8.25&5.43&4.38&0.24&0.5\\\hline
19&26.34 $\pm$ 2.06E-02& 8.23 $\pm$ 1.82E-03&6.26&4.77&4.26&6.29&4.8&4.26&0.24&0.42\\\hline
20&27.55 $\pm$ 3.19E-02& 19.41 $\pm$ 1.03E-02&15.05&13.74&13.69&15.59&14.27&13.33&0.22&0.28\\\hline
21&26.53 $\pm$ 2.42E-02& 10.66 $\pm$ 3.25E-03&8.26&6.57&5.78&8.24&6.68&5.57&0.22&0.38\\\hline
22&30.31 $\pm$ 2.02E-02& 7.42 $\pm$ 1.58E-03&6.86&6.68&6.46&6.79&6.62&6.74&0.08&0.1\\\hline
23&24.41 $\pm$ 2.02E-02& 7.92 $\pm$ 1.68E-03&5.6&4.84&5.22&5.57&4.81&5.14&0.29&0.39\\\hline
24&26.04 $\pm$ 2.24E-02& 9.21 $\pm$ 2.41E-03&6.98&5.72&5.55&7&5.7&5.5&0.24&0.38\\\hline
25&31.40 $\pm$ 2.15E-02& 7.46 $\pm$ 1.79E-03&7.12&6.59&6
TODO.32&7.04&6.55&6.29&0.04&0.11\\\hline
26&30.33 $\pm$ 2.85E-02& 15.32 $\pm$ 6.46E-03&12.44&9.9&8.05&12.2&9.72&7.92&0.19&0.36\\\hline
27&31.06 $\pm$ 2.28E-02& 9.42 $\pm$ 2.55E-03&8.81&8.8&8.36&8.97&8.96&8.64&0.06&0.06\\\hline
28&34.94 $\pm$ 4.63E-02& 26.74 $\pm$ 2.98E-02&26.29&25.57&23.72&26.35&25.65&23.84&0.02&0.04\\\hline
29&23.98 $\pm$ 3.49E-02& 20.02 $\pm$ 1.27E-02&16.96&14.4&11.75&16.92&14.33&11.66&0.15&0.28\\\hline
\end{tabular}
\caption{All numeric data captured in this paper. Includes sample mean and variance for each region as well as $r^2$, training, and test error for Linear (L), Nonlinear (NL), and Nonparametric (NP) models}
\label{resultstable}
\end{table}
\end{landscape}

\end{document}